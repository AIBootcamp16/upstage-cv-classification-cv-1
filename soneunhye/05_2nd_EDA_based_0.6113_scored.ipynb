{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%pip install timm albumentations -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹œë“œ ê³ ì •\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config (EDA ê¸°ë°˜ ìµœì í™”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    # ë°ì´í„°\n",
        "    'IMG_SIZE': 384,\n",
        "    'BATCH_SIZE': 8,\n",
        "    'NUM_CLASSES': 17,\n",
        "    \n",
        "    # í•™ìŠµ\n",
        "    'EPOCHS': 15,              # ë¹ ë¥¸ ì‹¤í—˜\n",
        "    'LR': 2.5e-5,\n",
        "    'WEIGHT_DECAY': 1e-5,\n",
        "    \n",
        "    # ëª¨ë¸\n",
        "    'MODEL_NAME': 'efficientnet_b4',  # ê²€ì¦ë¨\n",
        "    \n",
        "    # ê²½ë¡œ\n",
        "    'BASE_DIR': '/home/realtheai/cv_competetion',\n",
        "    'TRAIN_CSV': '/home/realtheai/cv_competetion/data/train.csv',\n",
        "    'TEST_CSV': '/home/realtheai/cv_competetion/data/sample_submission.csv',\n",
        "    'TRAIN_IMG_DIR': '/home/realtheai/cv_competetion/data/train',\n",
        "    'TEST_IMG_DIR': '/home/realtheai/cv_competetion/data/test',\n",
        "    \n",
        "    # ë””ë°”ì´ìŠ¤\n",
        "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "    # Augmentation (EDA ê¸°ë°˜!)\n",
        "    'USE_CLASS_WEIGHTS': True,\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“Š Configuration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"ëª¨ë¸: {CFG['MODEL_NAME']}\")\n",
        "print(f\"IMG_SIZE: {CFG['IMG_SIZE']}\")\n",
        "print(f\"BATCH_SIZE: {CFG['BATCH_SIZE']}\")\n",
        "print(f\"EPOCHS: {CFG['EPOCHS']}\")\n",
        "print(f\"Device: {CFG['DEVICE']}\")\n",
        "print(\"=\"*60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset & Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Augmentation (EDA ê¸°ë°˜ ìµœì í™”!)\n",
        "trn_transform = A.Compose([\n",
        "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
        "    \n",
        "    # ğŸ”¥ ì„ ëª…ë„ ì°¨ì´ ëŒ€ì‘ (ìµœìš°ì„ !)\n",
        "    A.GaussianBlur(blur_limit=(3, 9), p=0.6),  # ê°•í•˜ê²Œ!\n",
        "    A.MotionBlur(blur_limit=7, p=0.3),\n",
        "    \n",
        "    # ë¬¸ì„œ íŠ¹í™” (íšŒì „ì€ ì•½í•˜ê²Œ)\n",
        "    A.Rotate(limit=3, p=0.3),\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.05,\n",
        "        scale_limit=0.1,\n",
        "        rotate_limit=3,\n",
        "        p=0.3\n",
        "    ),\n",
        "    \n",
        "    # ë°ê¸° (ì°¨ì´ ì‘ìŒ, ì ë‹¹íˆ)\n",
        "    A.RandomBrightnessContrast(\n",
        "        brightness_limit=0.3,\n",
        "        contrast_limit=0.2,\n",
        "        p=0.5\n",
        "    ),\n",
        "    \n",
        "    # í’ˆì§ˆ ì €í•˜\n",
        "    A.ImageCompression(quality_lower=60, p=0.3),\n",
        "    A.Downscale(scale_min=0.75, scale_max=0.95, p=0.2),\n",
        "    \n",
        "    # ë…¸ì´ì¦ˆ\n",
        "    A.GaussNoise(var_limit=(10, 50), p=0.3),\n",
        "    \n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "tst_transform = A.Compose([\n",
        "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, csv_path, img_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['ID'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = np.array(img)\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']\n",
        "        \n",
        "        if 'target' in self.df.columns:\n",
        "            target = self.df.iloc[idx]['target']\n",
        "            return img, target\n",
        "        return img\n",
        "\n",
        "print(\"âœ… Dataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
        "        in_features = self.backbone.num_features\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "print(\"âœ… Model í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Train/Valid í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    preds_list, targets_list = [], []\n",
        "    \n",
        "    for imgs, targets in tqdm(loader, desc=\"Train\"):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds_list.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        targets_list.extend(targets.cpu().numpy())\n",
        "    \n",
        "    acc = accuracy_score(targets_list, preds_list)\n",
        "    f1 = f1_score(targets_list, preds_list, average='macro')\n",
        "    \n",
        "    return total_loss / len(loader), acc, f1\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    preds_list, targets_list = [], []\n",
        "    \n",
        "    for imgs, targets in tqdm(loader, desc=\"Valid\"):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        \n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds_list.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        targets_list.extend(targets.cpu().numpy())\n",
        "    \n",
        "    acc = accuracy_score(targets_list, preds_list)\n",
        "    f1 = f1_score(targets_list, preds_list, average='macro')\n",
        "    \n",
        "    return total_loss / len(loader), acc, f1, preds_list, targets_list\n",
        "\n",
        "print(\"âœ… Train/Valid í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "train_df = pd.read_csv(CFG['TRAIN_CSV'])\n",
        "\n",
        "# Train/Valid Split\n",
        "train_data, valid_data = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.2,\n",
        "    stratify=train_df['target'],\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Dataset & DataLoader\n",
        "trn_dataset = ImageDataset(CFG['TRAIN_CSV'], CFG['TRAIN_IMG_DIR'], trn_transform)\n",
        "trn_dataset.df = train_data.reset_index(drop=True)\n",
        "\n",
        "val_dataset = ImageDataset(CFG['TRAIN_CSV'], CFG['TRAIN_IMG_DIR'], tst_transform)\n",
        "val_dataset.df = valid_data.reset_index(drop=True)\n",
        "\n",
        "tst_dataset = ImageDataset(CFG['TEST_CSV'], CFG['TEST_IMG_DIR'], tst_transform)\n",
        "\n",
        "trn_loader = DataLoader(trn_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4)\n",
        "tst_loader = DataLoader(tst_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"âœ… Train: {len(trn_dataset)} | Valid: {len(val_dataset)} | Test: {len(tst_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model = DocumentClassifier(CFG['MODEL_NAME'], CFG['NUM_CLASSES']).to(CFG['DEVICE'])\n",
        "\n",
        "# Class Weights (ë¶ˆê· í˜• ëŒ€ì‘)\n",
        "if CFG['USE_CLASS_WEIGHTS']:\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['target']), y=train_df['target'])\n",
        "    class_weights = torch.FloatTensor(class_weights).to(CFG['DEVICE'])\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    print(\"âœ… Weighted CrossEntropyLoss\")\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"âœ… CrossEntropyLoss\")\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = Adam(model.parameters(), lr=CFG['LR'], weight_decay=CFG['WEIGHT_DECAY'])\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=1e-6)\n",
        "\n",
        "print(f\"âœ… Model: {CFG['MODEL_NAME']}\")\n",
        "print(f\"âœ… Optimizer: Adam (LR={CFG['LR']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training (15 Epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸš€ Training ì‹œì‘!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_f1 = 0\n",
        "best_epoch = 0\n",
        "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
        "\n",
        "for epoch in range(1, CFG['EPOCHS'] + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{CFG['EPOCHS']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Train\n",
        "    trn_loss, trn_acc, trn_f1 = train_one_epoch(model, trn_loader, criterion, optimizer, CFG['DEVICE'])\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc, val_f1, val_preds, val_targets = validate(model, val_loader, criterion, CFG['DEVICE'])\n",
        "    \n",
        "    # Scheduler\n",
        "    scheduler.step()\n",
        "    \n",
        "    # History\n",
        "    history['train_loss'].append(trn_loss)\n",
        "    history['train_f1'].append(trn_f1)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"Train Loss: {trn_loss:.4f} | F1: {trn_f1:.4f}\")\n",
        "    print(f\"Valid Loss: {val_loss:.4f} | F1: {val_f1:.4f}\")\n",
        "    \n",
        "    # Best ëª¨ë¸ ì €ì¥\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), f\"{CFG['BASE_DIR']}/06_best_model.pth\")\n",
        "        print(f\"âœ… Best F1: {best_f1:.4f} (Saved!)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"âœ… Training ì™„ë£Œ!\")\n",
        "print(f\"   Best F1: {best_f1:.4f} (Epoch {best_epoch})\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµ ê³¡ì„ \n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Valid')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# F1 Score\n",
        "axes[1].plot(history['train_f1'], label='Train')\n",
        "axes[1].plot(history['val_f1'], label='Valid')\n",
        "axes[1].axhline(best_f1, color='red', linestyle='--', label=f'Best: {best_f1:.4f}')\n",
        "axes[1].set_title('F1 Score')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{CFG['BASE_DIR']}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrong Predictions Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_wrong_predictions(val_df, preds_list, targets_list, img_dir, meta_df=None, save_path=None):\n",
        "    \"\"\"í‹€ë¦° ì´ë¯¸ì§€ ì‹œê°í™” ë° íŒ¨í„´ ë¶„ì„\"\"\"\n",
        "    # í‹€ë¦° ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "    wrong_indices = [i for i, (pred, target) in enumerate(zip(preds_list, targets_list)) if pred != target]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ“Š ê²€ì¦ ê²°ê³¼ ë¶„ì„\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"   ì „ì²´: {len(preds_list)}ê°œ\")\n",
        "    print(f\"   ì •ë‹µ: {len(preds_list) - len(wrong_indices)}ê°œ\")\n",
        "    print(f\"   ì˜¤ë‹µ: {len(wrong_indices)}ê°œ ({len(wrong_indices)/len(preds_list)*100:.1f}%)\")\n",
        "    print(f\"   ì •í™•ë„: {(len(preds_list) - len(wrong_indices))/len(preds_list)*100:.1f}%\")\n",
        "    \n",
        "    if len(wrong_indices) == 0:\n",
        "        print(\"\\nğŸ‰ ëª¨ë“  ì˜ˆì¸¡ì´ ì •í™•í•©ë‹ˆë‹¤!\")\n",
        "        return\n",
        "    \n",
        "    # í´ë˜ìŠ¤ë³„ ì˜¤ë¥˜ ë¶„ì„\n",
        "    wrong_by_true_class = {}\n",
        "    for idx in wrong_indices:\n",
        "        true_class = targets_list[idx]\n",
        "        if true_class not in wrong_by_true_class:\n",
        "            wrong_by_true_class[true_class] = []\n",
        "        wrong_by_true_class[true_class].append(idx)\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì˜¤ë‹µ ë¶„ì„:\")\n",
        "    for class_id in sorted(wrong_by_true_class.keys()):\n",
        "        count = len(wrong_by_true_class[class_id])\n",
        "        if meta_df is not None:\n",
        "            class_name = meta_df[meta_df['target'] == class_id]['class_name'].values[0]\n",
        "            print(f\"   Class {class_id:2d} ({class_name[:20]:20s}): {count:2d}ê°œ ì˜¤ë‹µ\")\n",
        "        else:\n",
        "            print(f\"   Class {class_id:2d}: {count:2d}ê°œ ì˜¤ë‹µ\")\n",
        "    \n",
        "    # í‹€ë¦° ì´ë¯¸ì§€ ì‹œê°í™” (12ê°œ)\n",
        "    if save_path:\n",
        "        n_show = min(12, len(wrong_indices))\n",
        "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        for i, idx in enumerate(wrong_indices[:n_show]):\n",
        "            img_name = val_df.iloc[idx]['ID']\n",
        "            img_path = os.path.join(img_dir, img_name)\n",
        "            \n",
        "            if os.path.exists(img_path):\n",
        "                img = Image.open(img_path)\n",
        "                axes[i].imshow(img)\n",
        "                \n",
        "                true_label = f\"True: {targets_list[idx]}\"\n",
        "                pred_label = f\"Pred: {preds_list[idx]}\"\n",
        "                \n",
        "                if meta_df is not None:\n",
        "                    true_class = meta_df[meta_df['target'] == targets_list[idx]]['class_name'].values[0]\n",
        "                    pred_class = meta_df[meta_df['target'] == preds_list[idx]]['class_name'].values[0]\n",
        "                    true_label += f\"\\n({true_class[:15]})\"\n",
        "                    pred_label += f\"\\n({pred_class[:15]})\"\n",
        "                \n",
        "                axes[i].set_title(\n",
        "                    f\"{true_label}\\n{pred_label}\",\n",
        "                    fontsize=9,\n",
        "                    color='red',\n",
        "                    fontweight='bold'\n",
        "                )\n",
        "                axes[i].axis('off')\n",
        "        \n",
        "        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°\n",
        "        for i in range(n_show, 12):\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"\\nğŸ’¾ í‹€ë¦° ì´ë¯¸ì§€ ì €ì¥: {save_path}\")\n",
        "\n",
        "print(\"âœ… analyze_wrong_predictions í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# meta.csv ë¡œë“œ\n",
        "meta_df = pd.read_csv(f\"{CFG['BASE_DIR']}/data/meta.csv\")\n",
        "\n",
        "# Wrong Predictions ë¶„ì„\n",
        "analyze_wrong_predictions(\n",
        "    val_dataset.df,\n",
        "    val_preds,\n",
        "    val_targets,\n",
        "    CFG['TRAIN_IMG_DIR'],\n",
        "    meta_df=meta_df,\n",
        "    save_path=f\"{CFG['BASE_DIR']}/wrong_predictions.png\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(val_targets, val_preds)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# ì¶• ë ˆì´ë¸” (í´ë˜ìŠ¤ ì´ë¦„)\n",
        "tick_labels = [f\"{i}\\n{meta_df[meta_df['target']==i]['class_name'].values[0][:10]}\" \n",
        "               for i in range(CFG['NUM_CLASSES'])]\n",
        "plt.xticks(np.arange(CFG['NUM_CLASSES']) + 0.5, tick_labels, rotation=45, ha='right', fontsize=8)\n",
        "plt.yticks(np.arange(CFG['NUM_CLASSES']) + 0.5, tick_labels, fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{CFG['BASE_DIR']}/confusion_matrix.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬\n",
        "print(f\"\\nğŸ“ˆ Validation í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:\")\n",
        "val_pred_counts = pd.Series(val_preds).value_counts().sort_index()\n",
        "for class_id, count in val_pred_counts.items():\n",
        "    class_name = meta_df[meta_df['target'] == class_id]['class_name'].values[0]\n",
        "    print(f\"   Class {class_id:2d} ({class_name[:25]:25s}): {count:3d}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference & Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best ëª¨ë¸ ë¡œë“œ\n",
        "model.load_state_dict(torch.load(f\"{CFG['BASE_DIR']}/06_best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Test ì˜ˆì¸¡\n",
        "preds_list = []\n",
        "for imgs, _ in tqdm(tst_loader, desc=\"Inference\"):\n",
        "    imgs = imgs.to(CFG['DEVICE'])\n",
        "    outputs = model(imgs)\n",
        "    preds_list.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission = pd.read_csv(CFG['TEST_CSV'])\n",
        "submission['target'] = preds_list\n",
        "submission.to_csv(f\"{CFG['BASE_DIR']}/submission.csv\", index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
